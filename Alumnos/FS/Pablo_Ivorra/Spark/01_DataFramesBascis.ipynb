{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3WKxUMvQ9JW"
      },
      "source": [
        "# DataFrames Basics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBsRl4yARA3x"
      },
      "source": [
        "## Prerrequisites"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KID1ObYRDA4"
      },
      "source": [
        "Install Spark and Java in VM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-9EjQzqhRJSZ"
      },
      "outputs": [],
      "source": [
        "# install Java8\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "# download spark 3.5.0\n",
        "!wget -q https://apache.osuosl.org/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E4s6wcUsSPHQ",
        "outputId": "711732bb-89ef-4b6a-8948-427ae879a3e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 782028\n",
            "drwxr-xr-x 1 root root      4096 Jan 10 14:23 \u001b[0m\u001b[01;34msample_data\u001b[0m/\n",
            "-rw-r--r-- 1 root root 400395283 Sep  9 02:10 spark-3.5.0-bin-hadoop3.tgz\n",
            "-rw-r--r-- 1 root root 400395283 Sep  9 02:10 spark-3.5.0-bin-hadoop3.tgz.1\n"
          ]
        }
      ],
      "source": [
        "ls -l # check the .tgz is there"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hSA_T2q7SEt_"
      },
      "outputs": [],
      "source": [
        "# unzip it\n",
        "!tar xf spark-3.5.0-bin-hadoop3.tgz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HpKEfJTeii2Y"
      },
      "outputs": [],
      "source": [
        "!pip install -q findspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zwH-zC17SGnP",
        "outputId": "04c25965-4138-4009-d2fe-b807a7081d61"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: py4j in /usr/local/lib/python3.10/dist-packages (0.10.9.7)\n",
            "Requirement already satisfied: folium in /usr/local/lib/python3.10/dist-packages (0.14.0)\n",
            "Requirement already satisfied: branca>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from folium) (0.7.0)\n",
            "Requirement already satisfied: jinja2>=2.9 in /usr/local/lib/python3.10/dist-packages (from folium) (3.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from folium) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from folium) (2.31.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.9->folium) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->folium) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->folium) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->folium) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->folium) (2023.11.17)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (5.15.0)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly) (8.2.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from plotly) (23.2)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install py4j\n",
        "\n",
        "# For maps\n",
        "!pip install folium\n",
        "!pip install plotly"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1tk452JRjuY"
      },
      "source": [
        "Define the environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vrMxCiuZRl7h"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.0-bin-hadoop3\"\n",
        "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = \"--master local[*] pyspark-shell\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vk_3rL02Q9JZ"
      },
      "source": [
        "Start Spark Session\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "-HtQz6mfQ9JZ",
        "outputId": "305f8865-3826-401c-e586-772cd0ad5c94"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'3.5.0'"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import findspark\n",
        "findspark.init(\"spark-3.5.0-bin-hadoop3\")# SPARK_HOME\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# create the session\n",
        "spark = SparkSession \\\n",
        "        .builder \\\n",
        "        .appName(\"DataFrames Basics\") \\\n",
        "        .master(\"local[*]\") \\\n",
        "        .getOrCreate()\n",
        "\n",
        "spark.version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "DrOjmfL9Q9Ja",
        "outputId": "4506e7c5-e86b-43a1-d6ae-1d31d5e3ad1a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://049af5ed9523:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.0</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>DataFrames Basics</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x78aa38128130>"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IBZ8ufPAQ9Jb"
      },
      "outputs": [],
      "source": [
        "# For Pandas conversion optimization\n",
        "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nJ-cdrAlQ9Jb"
      },
      "outputs": [],
      "source": [
        "# Import sql functions\n",
        "from pyspark.sql.functions import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIP7U3YYTDbw"
      },
      "source": [
        "Download datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d63cazZLTKTv",
        "outputId": "d16103a9-f792-41b1-d1b5-d7406b5c90a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "bank.csv  cars.json  movies.json  vehicles.csv\n"
          ]
        }
      ],
      "source": [
        "!mkdir -p dataset\n",
        "!wget -q https://raw.githubusercontent.com/paponsro/spark_edem_2324/master/dataset/cars.json -P /dataset\n",
        "!wget -q https://raw.githubusercontent.com/paponsro/spark_edem_2324/master/dataset/movies.json -P /dataset\n",
        "!wget -q https://github.com/masfworld/datahack_docker/raw/master/zeppelin/data/bank.csv -P /dataset\n",
        "!wget -q https://github.com/masfworld/datahack_docker/raw/master/zeppelin/data/vehicles.csv -P /dataset\n",
        "!ls /dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ml1O2OiQU6ah",
        "outputId": "8ed0b279-abb0-4c42-a5c0-fae5f3894aa4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 1784\n",
            "-rw-r--r-- 1 root root  461474 Jan 12 17:23 bank.csv\n",
            "-rw-r--r-- 1 root root   74910 Jan 12 17:23 cars.json\n",
            "-rw-r--r-- 1 root root 1274347 Jan 12 17:23 movies.json\n",
            "-rw-r--r-- 1 root root    4370 Jan 12 17:23 vehicles.csv\n"
          ]
        }
      ],
      "source": [
        "ls -l /dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94aRTBiPQ9Jc"
      },
      "source": [
        "## Examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XxXiDT4XUgjl"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.types import Row\n",
        "from pyspark.sql.functions import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p69TjjJBTrDF",
        "outputId": "3f6f9140-de41-4e6c-bf77-f59e99a871d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+----------+-------+---------+-------+\n",
            "|age|       job|marital|education|balance|\n",
            "+---+----------+-------+---------+-------+\n",
            "| 30|unemployed|married|  primary|   1787|\n",
            "| 33|  services|married|secondary|   4789|\n",
            "| 35|management| single| tertiary|   1350|\n",
            "+---+----------+-------+---------+-------+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "bankText = spark.sparkContext.textFile(\"/dataset/bank.csv\")\n",
        "\n",
        "#we have to: remove firt row (headers), map the rest, and create DF\n",
        "bank = bankText.map(lambda lineaCsv: lineaCsv.split(\";\"))\\\n",
        ".filter(lambda s: s[0] != \"\\\"age\\\"\") \\\n",
        ".map(lambda row: Row(int(row[0]), row[1].replace(\"\\\"\", \"\"), row[2].replace(\"\\\"\", \"\"), row[3].replace(\"\\\"\", \"\"), row[5].replace(\"\\\"\", \"\"))) \\\n",
        ".toDF([\"age\", \"job\", \"marital\", \"education\", \"balance\"]) \\\n",
        ".withColumn(\"age\", col(\"age\").cast(\"int\"))\n",
        "\n",
        "bank.show(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DnCG9XHsTUfe"
      },
      "source": [
        "Read directly from JSON file to a DF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iBWBcsLDTYuF"
      },
      "outputs": [],
      "source": [
        "carsDF = spark.read.option(\"inferSchema\", True).json(\"/dataset/cars.json\") # inferSchema requires one extra pass over the data\n",
        "\n",
        "# if None is set, it uses de default value (default = False) you can also pass the schema manually"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beN35oUrUo4c"
      },
      "source": [
        "Read directly from csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xaxDO13vUlSs",
        "outputId": "6161ba3e-b33e-4290-872a-e9b3bdaaaa0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+----------+-------+---------+-------+-------+-------+----+--------+---+-----+--------+--------+-----+--------+--------+---+\n",
            "|age|       job|marital|education|default|balance|housing|loan| contact|day|month|duration|campaign|pdays|previous|poutcome|  y|\n",
            "+---+----------+-------+---------+-------+-------+-------+----+--------+---+-----+--------+--------+-----+--------+--------+---+\n",
            "| 30|unemployed|married|  primary|     no|   1787|     no|  no|cellular| 19|  oct|      79|       1|   -1|       0| unknown| no|\n",
            "| 33|  services|married|secondary|     no|   4789|    yes| yes|cellular| 11|  may|     220|       1|  339|       4| failure| no|\n",
            "| 35|management| single| tertiary|     no|   1350|    yes|  no|cellular| 16|  apr|     185|       1|  330|       1| failure| no|\n",
            "+---+----------+-------+---------+-------+-------+-------+----+--------+---+-----+--------+--------+-----+--------+--------+---+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "bankDF = spark.read.option(\"header\", \"true\").option(\"delimiter\", \";\").csv(\"/dataset/bank.csv\")\n",
        "bankDF.show(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOt52pbaQ9Jc"
      },
      "source": [
        "Showing a DF and print schema"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PmmsrRxaQ9Jd",
        "outputId": "f67397e9-53cd-437f-c4a2-c2bacc460c3b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------+---------+------------+----------+----------------+--------------------+------+-------------+----------+\n",
            "|Acceleration|Cylinders|Displacement|Horsepower|Miles_per_Gallon|                Name|Origin|Weight_in_lbs|      Year|\n",
            "+------------+---------+------------+----------+----------------+--------------------+------+-------------+----------+\n",
            "|        12.0|        8|       307.0|       130|            18.0|chevrolet chevell...|   USA|         3504|1970-01-01|\n",
            "|        11.5|        8|       350.0|       165|            15.0|   buick skylark 320|   USA|         3693|1970-01-01|\n",
            "+------------+---------+------------+----------+----------------+--------------------+------+-------------+----------+\n",
            "only showing top 2 rows\n",
            "\n",
            "root\n",
            " |-- Acceleration: double (nullable = true)\n",
            " |-- Cylinders: long (nullable = true)\n",
            " |-- Displacement: double (nullable = true)\n",
            " |-- Horsepower: long (nullable = true)\n",
            " |-- Miles_per_Gallon: double (nullable = true)\n",
            " |-- Name: string (nullable = true)\n",
            " |-- Origin: string (nullable = true)\n",
            " |-- Weight_in_lbs: long (nullable = true)\n",
            " |-- Year: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "carsDF.show(2)\n",
        "carsDF.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ij4TDMZ0lkBZ"
      },
      "source": [
        "Get Rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SB06W84wlnlY",
        "outputId": "4b58abb9-d50e-44b8-f23a-17d8e915fc98"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Row(Acceleration=12.0, Cylinders=8, Displacement=307.0, Horsepower=130, Miles_per_Gallon=18.0, Name='chevrolet chevelle malibu', Origin='USA', Weight_in_lbs=3504, Year='1970-01-01'),\n",
              " Row(Acceleration=11.5, Cylinders=8, Displacement=350.0, Horsepower=165, Miles_per_Gallon=15.0, Name='buick skylark 320', Origin='USA', Weight_in_lbs=3693, Year='1970-01-01')]"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "carsDF.take(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z89eHF1aQ9Jd"
      },
      "source": [
        "Count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rtFilGlZWGyo",
        "outputId": "fb46bb1b-8b51-4204-82b1-201efc07830c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "406"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "carsDF.count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akmA70EPQ9Jd"
      },
      "source": [
        "Schema"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rp6hKanzQ9Je",
        "outputId": "e2254584-d07c-4be1-e92a-860425620ea1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pyspark.sql.types.StructType'>\n",
            "StructType([StructField('Acceleration', DoubleType(), True), StructField('Cylinders', LongType(), True), StructField('Displacement', DoubleType(), True), StructField('Horsepower', LongType(), True), StructField('Miles_per_Gallon', DoubleType(), True), StructField('Name', StringType(), True), StructField('Origin', StringType(), True), StructField('Weight_in_lbs', LongType(), True), StructField('Year', StringType(), True)])\n"
          ]
        }
      ],
      "source": [
        "# obtain a schema\n",
        "carsSchema = carsDF.schema\n",
        "print(type(carsSchema))\n",
        "print(carsSchema)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_XS0vB0Q9Je"
      },
      "source": [
        "Custom Schemas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ekQkJLT8Q9Je"
      },
      "outputs": [],
      "source": [
        "example = spark.sparkContext.parallelize([(\"chevrolet chevelle malibu\",18,\"1970-01-01\",\"USA\"),\n",
        "    (\"buick skylark 320\",15,\"1970-01-01\",\"USA\")])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nUzjX4kTQ9Je",
        "outputId": "92e52101-1a92-4906-8cc3-62a5c634ca88"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- _1: string (nullable = true)\n",
            " |-- _2: long (nullable = true)\n",
            " |-- _3: string (nullable = true)\n",
            " |-- _4: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "exampleDF = spark.createDataFrame(example)\n",
        "exampleDF.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GeTtjRirQ9Je"
      },
      "source": [
        "With columns names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uhY-f3j1Q9Jf"
      },
      "outputs": [],
      "source": [
        "names = list([\"name\", \"weight\", \"date\", \"country\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nqxqPyr1Q9Jf",
        "outputId": "60c18260-0a43-43bb-eb1e-4fa55ce35f28"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- name: string (nullable = true)\n",
            " |-- weight: long (nullable = true)\n",
            " |-- date: string (nullable = true)\n",
            " |-- country: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "example2DF = example.toDF(names)\n",
        "example2DF.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "toUIbosSQ9Jf"
      },
      "outputs": [],
      "source": [
        "# importing sql types\n",
        "from pyspark.sql.types import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KZNfLhFWQ9Jf"
      },
      "outputs": [],
      "source": [
        "# custom schema\n",
        "customSchema = StructType([ \\\n",
        "    StructField('name', StringType(), True), \\\n",
        "    StructField('weight', StringType(), True), \\\n",
        "    StructField('date', StringType(), True), \\\n",
        "    StructField('country', StringType(), True)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x63bbTDYQ9Jf",
        "outputId": "bb51df6c-080f-4790-ebed-7df150072a61"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- name: string (nullable = true)\n",
            " |-- weight: string (nullable = true)\n",
            " |-- date: string (nullable = true)\n",
            " |-- country: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "example3DF = spark.createDataFrame(example, customSchema)\n",
        "example3DF.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRiqa-G5Q9Jf",
        "outputId": "d36f26b5-c91e-42a2-8c1a-fb24d575f514"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------------------+------+----------+-------+\n",
            "|name                     |weight|date      |country|\n",
            "+-------------------------+------+----------+-------+\n",
            "|chevrolet chevelle malibu|18    |1970-01-01|USA    |\n",
            "|buick skylark 320        |15    |1970-01-01|USA    |\n",
            "+-------------------------+------+----------+-------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "example3DF.show(2, False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M3qToymLQ9Jg"
      },
      "outputs": [],
      "source": [
        "# we can also specify schema with DDL (Data Definition Language)\n",
        "customSchema2 = \"`name` STRING NOT NULL, `weight` INT, `date` STRING, `country` STRING\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7k9X1gSSQ9Jg",
        "outputId": "7f5ae0d4-3fce-4b7d-a8ba-b92968981e1a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- name: string (nullable = false)\n",
            " |-- weight: integer (nullable = true)\n",
            " |-- date: string (nullable = true)\n",
            " |-- country: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "example4DF = spark.createDataFrame(example, customSchema2)\n",
        "example4DF.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5rayh3uQ9Jg",
        "outputId": "2c96f107-9a8b-48fa-e27d-798f91745f28"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'int'>\n",
            "<class 'str'>\n"
          ]
        }
      ],
      "source": [
        "print(type(example2DF.collect()[0][\"weight\"]))\n",
        "print(type(example3DF.collect()[0][\"weight\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QnciaoWQ9Jg"
      },
      "source": [
        "## Exercises\n",
        "1) Create a manual DF describing smartphones\n",
        "  - maker\n",
        "  - model\n",
        "  - screen dimension\n",
        "  - camera megapixels\n",
        "  \n",
        "2) Read another file from the dataset/ folder, e.g. movies.json\n",
        "  - print its schema\n",
        "  - count the number of rows, call count()\n",
        "\n",
        "3) Take a look to vehicles.csv. Read the file to a DF but this time with your own schema"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQR1cduXQ9Jg"
      },
      "source": [
        "Exercise 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "smartphones = spark.sparkContext.parallelize([(\"iphone\",4,\"120x120\",\"50mpx\"),\n",
        "    (\"samsung\",5,\"50x60\",\"60mpx\")])\n",
        "\n",
        "\n",
        "\n",
        "customSchema = StructType([ \\\n",
        "    StructField('maker', StringType(), True), \\\n",
        "    StructField('model', StringType(), True), \\\n",
        "    StructField('screen dimension', StringType(), True), \\\n",
        "    StructField('camera megapixels', StringType(), True)])\n",
        "\n",
        "\n",
        "\n",
        "smartphones = spark.createDataFrame(smartphones,customSchema)\n",
        "smartphones.printSchema()\n",
        "smartphones.show(2, False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zmg5kbOpl-s3"
      },
      "source": [
        "Exercise 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "moviesDF = spark.read.option(\"inferSchema\", True).json(\"/dataset/movies.json\")\n",
        "\n",
        "moviesSchema = moviesDF.schema\n",
        "print(type(moviesSchema))\n",
        "print(moviesSchema)\n",
        "moviesDF.printSchema()\n",
        "\n",
        "moviesDF.count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAqADV5jW00d"
      },
      "source": [
        "Exercise 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JD2KQtXiYmVn"
      },
      "outputs": [],
      "source": [
        "\n",
        "vehiclesDF = spark.read.option(\"header\", \"true\").option(\"delimiter\", \";\").schema(customSchema).csv(\"/dataset/vehicles.csv\")\n",
        "vehiclesDF.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "94aRTBiPQ9Jc"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.0 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "ff1af5cda0bea4fe5c4ebc1f94ab9f13d8998f98d08e16d8aba48673b9d00116"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
