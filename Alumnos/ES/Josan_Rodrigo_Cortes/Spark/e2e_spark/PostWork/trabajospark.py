# -*- coding: utf-8 -*-
"""TrabajoSpark.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FJfnEtU1T5WK4AGaUsmV-ulhoJ3lI_Za

# Windows Partitioning

## Prerrequisites

Install Spark and Java in VM
"""

# # install Java8
# !apt-get install openjdk-8-jdk-headless -qq > /dev/null
# # download spark 3.5.0
# !wget -q https://apache.osuosl.org/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz

# ls -l # check the .tgz is there

# # unzip it
# !tar xf spark-3.5.0-bin-hadoop3.tgz

# !pip install -q findspark

"""Defining the environment"""

import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-3.5.0-bin-hadoop3"
os.environ["PYSPARK_SUBMIT_ARGS"] = "--master local[*] pyspark-shell"

"""Start Spark Session

---
"""

import findspark
findspark.init("spark-3.5.0-bin-hadoop3")# SPARK_HOME

from pyspark.sql import SparkSession

# create the session
spark = SparkSession \
        .builder \
        .appName("Window Partitioning") \
        .master("local[*]") \
        .getOrCreate()

spark.version

spark

# For Pandas conversion optimization
spark.conf.set("spark.sql.execution.arrow.enabled", "true")

# Import sql functions
from pyspark.sql.functions import *

from google.colab import drive
drive.mount('/content/drive')

"""Cargo el csv en la variable realStateDF, lo muestro y veo el esquema."""

realStateDF= spark.read.option("header", "true").option("delimiter", ";").csv("/content/drive/MyDrive/Real Estate Dataset.csv")
realStateDF.show(3)
realStateDF.printSchema()

"""Muestro la media y la desviación típica del precio de las viviendas de la muestra:"""

realStateDF.select(avg(realStateDF.price)).show()
realStateDF.select(stddev(realStateDF.price)).show()

"""Filtro las viviendas del dataset que estan en 'Original condition'"""

realStatefiltradoDF=realStateDF.filter(col("condition")=="Original condition")
realStatefiltradoDF.show(3)

"""Numero de inmuebles por condición de estado:


"""

realStateareasDF = realStateDF.groupBy(realStateDF.condition).count().orderBy("count")
realStateareasDF.show()

"""Numero inmuebles por numero de habitaciones"""

realStateroomsDF = realStateDF.groupBy(realStateDF.rooms).count().orderBy("count")
realStateroomsDF.show()

"""Creo la columna fila_enum para poder crear otro dataset a partir del original y poder hacer un left-join"""

realStateDF.printSchema()

realStateDF = realStateDF.withColumn("fila_enum", monotonically_increasing_id())
realStateDF.show(3)

"""Creo otro dataset ficticio a partir del primero para poder hacer join. Agrego a ambos datasets el fila_enum para poder hacer el join por la coincidencia. Solo he multiplicado la columna 'rooms' y 'price' por dos."""

realStateCopiaDF=realStateDF.withColumn("segundoPrecio", expr("price * 2")).withColumn("segundosRooms", expr("rooms * 2"))
realStateCopiaDF.show(3)
columnasAeliminar= ['name_nsi','price','index','environment','quality_of_living','safety','transport','services','relax','condition','area','energy_costs','provision','certificate','construction_type','orientation','year_built','last_reconstruction','total_floors','floor','lift','balkonies','loggia','cellar','type','rooms','district']
realStateCopiaDF=realStateCopiaDF.drop(*columnasAeliminar)
realStateCopiaDF = realStateCopiaDF.withColumn("fila_enum", monotonically_increasing_id())

realStateCopiaDF.show()

"""Uno ambos datasets, el original y el creado por la columna fila_enum"""

# joinCondition = realStateDF.fila_enum == realStateCopiaDF.fila_enum
nuevorealStateDF= realStateDF.join(realStateCopiaDF, 'fila_enum', "left_outer").orderBy("fila_enum").show(3)

"""Importo las funciones de pyspark como F y la función Window."""

from pyspark.sql import functions as F
from pyspark.sql.window import Window

"""Creo tres columnas gracias a la funcion window donde se muestra por cada elemento del DF el precio medio de ese tipo de vivienda en concreto, la media de habitaciones de ese tipo de vivienda y el total de inmuebles."""

realStateareasDF = realStateDF.groupBy("condition").count().orderBy("count")
realStateareasDF.show()

win = Window().partitionBy("condition").orderBy("price")

windowDF = realStateDF.withColumn('precio_medio', F.avg(("price")).over(win)).withColumn('Media_de_habitaciones',F.avg("rooms").over(win)).withColumn('Total_Inbuebles_de_este_tipo',F.count("condition").over(win))
windowDF.show()